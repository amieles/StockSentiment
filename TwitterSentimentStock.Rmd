---
title: "Twitter Sentiment Analysis"
output: html_document
---
In this project I will attempt to predict the price of TSLA, by analyzing sentiment
of tweets from the previous day. Tweets will be retrieved from twitter by API calls.
Tweets from the following Twitter accounts will be analyzed: @YahooFinance, @MarketWatch, @SeekingAlpha, @elonmusk.

Read in packages that are necessary for this project.
```{r packages}
library(tidyverse)
library(httr)
library(jsonlite)
library(rtweet)
library(tidyquant)
library(lubridate)
library(tidytext)Yes
```

```{r, include = FALSE}
key = "n5dd332tpKtUqrBaxkATqDmuw"
secret = "46NFbFOIje2Y3LsSDSoUSJXXn8McoFR2O0imJf1PkLKOujbTB1"
access = "1342016303595446272-aNhNUpp4cICZk3KnN1voKUjh7dOJrR"
access_secret = "MvcZsKgm3IfiQglB6WOyk0hGR9I7JmyxnQeYW0eKqa9gX"
app_name = "thee_app"
```
Connect to Twitter API with app details that are not included in this markdown output. 
```{r Token}
myToken = create_token(app = app_name,
             consumer_key = key, 
             consumer_secret = secret,
             access_token = access,
             access_secret = access_secret)
```

Use the API to retrieve up to 10,000 tweets from each account (10,000 is the max allowed, the API will likely provide less). Look at the head and lenght of this dataframe and decide which variables to keep. 
```{r}
tmls <- get_timelines(c("YahooFinance", "MarketWatch", "SeekingAlpha", "elonmusk"), n = 10000, include_rts = F)
head(tmls)
nrow(tmls)
tmls1 = tmls %>% select(screen_name, status_id, created_at, text, length = display_text_width, favorite_count, retweet_count) 
```
Write this dataframe to a csv file. 
```{r}
write_csv(tmls1, "ProjectTweets.csv")
```
Read in data set.
```{r}
t <- read_csv("ProjectTweets.csv")
```
Create a column that shows if a given tweet mentions Tesla. Text is converted to lowercase make sure all cases are identified. 
```{r, warning=FALSE}
t1 = t %>% mutate(text = str_to_lower(text))
t1 = t1 %>% mutate(ab_tesla = str_detect(text, "tesla | tsla")) 
t1 = t1 %>% mutate(status_id = as.character(status_id))
droptext = select(t1,-text)
```
Use the NRC lexicon to analyze the sentiment of tweets. Here we will read it in. This lexicon includes important words and their associated sentiment
```{r}
nrc_lexicon = get_sentiments("nrc")
```
Split each tweet into its individual words and remove unimportant words that do not convey sentiment. Now each word is a row. An inner join between the lexicon and the data frame is then done to keep words from the lexicon. This table is then transformed to use sentiments as columns, sorted by tweet Status ID for rows. 
```{r}
#find amount of each sentiment in every tweet
t2 = t1 %>% unnest_tokens(word, text) %>%  inner_join(nrc_lexicon, by = "word") %>% count(status_id, sentiment) %>%
  pivot_wider(names_from = sentiment, values_from = n, values_fill = 0) 

#join this table with other tweet stats from API call
t3 = t2 %>% right_join(droptext, by = "status_id")
#make dates into date type
t3 = t3 %>% mutate(created_at = as_date(created_at))
#change ab_tesla column into integers (1 for TRUE, 0 for FALSE)
t3 = t3 %>% mutate(TSLA = as.integer(ab_tesla))
t3 %>% head
```

```{r}
#aggregate all sentiment columns and TSLA variable according to date
t3 = mutate(t3, date = created_at)
sen_by_date = aggregate(cbind(TSLA, anticipation, positive, trust, fear, negative, sadness, disgust, joy, surprise, anger) ~ date , data = t3, sum, value_fill = 0) 
sen_by_date %>% head

```
```{r}
#retrieve stock prices with tidyquant
stock = 'TSLA' %>% tq_get(get = "stock.prices",
                  from = "2021-09-24",
                  to = "2021-11-13")
```
```{r}
#join stock price with daily tweet sentiment
final = sen_by_date %>% left_join(stock, by = "date")
final %>% head
```

```{r}
#add lagged variables for sentiments and drop NA rows 
final1 = final %>% mutate( TSLA1 = lag(TSLA), anticipation1 = lag(anticipation), positive1 = lag(positive), trust1 = lag(trust), fear1 = lag(fear), negative1 = lag(negative), sadness1 = lag(sadness), disgust1 = lag(disgust), joy1 = lag(joy), surprise1 = lag(surprise), anger1 = lag(anger)) %>% na.omit()
final1 %>% head
```
```{r}
t3
library(ggthemes)
ggplot(final1, aes(x = date, y= positive)) + geom_line() + ylab("Count") +xlab("") + ggtitle("Positive Tweet Sentiment") +theme_minimal()
```


```{r}
#converting dataframes into csv files
write_csv(t3, "../data/CompleteProjectTable.csv")
write_csv(final1, "../data/ProjectTable.csv")
```

```{r}
library(timetk)
library(modeltime)
library(tidymodels)
```

```{r}
#read in data for modeling
d = read_csv("../data/ProjectTable.csv")
d %>% head
d %>% plot_time_series(date, adjusted, .interactive = F)
```
```{r}
set.seed(8)
splits  = d %>% time_series_split(date_var = date, assess = "2 weeks",
                        cumulative = TRUE)
trs = training(splits)
ts = testing(splits)
trs %>% head
```

```{r}
basicRecipe = recipe(adjusted~ TSLA1 + anticipation1 + positive1 + trust1 + fear1 + negative1 + sadness1 + disgust1 + joy1 + surprise1 + anger1 + date, trs) 


basicRecipe %>% prep() %>% juice() 
```

```{r}
prophet_reg = prophet_reg() %>%
    set_engine("prophet") 
prophet_reg_b = prophet_boost() %>%
    set_engine("prophet_xgboost") 
arima_reg = arima_reg() %>% 
    set_engine("auto_arima")
arima_reg_b = arima_boost() %>% 
    set_engine("auto_arima_xgboost")
```

```{r}
bw = workflow() %>%  add_model(prophet_reg) %>% add_recipe(basicRecipe)
```

```{r}
m1 =  bw %>% fit(trs) 
m2 = bw %>% update_model(prophet_reg_b) %>%  fit(trs) 
m3 = bw %>% update_model(arima_reg) %>% fit(trs)
m4 = bw %>% update_model(arima_reg_b) %>% fit(trs)
```

```{r}
mt = modeltime_table(m1, m2, m3, m4)
ct = mt %>% modeltime_calibrate(ts)
```

```{r}
ct %>% modeltime_accuracy() %>% select(.model_id, 
  .model_desc , mae, rmse) %>% table_modeltime_accuracy(.interactive = F)
```

```{r}
ct %>% modeltime_forecast(new_data = ts, actual_data = trs) %>% 
  plot_modeltime_forecast(.interactive = F)
```

